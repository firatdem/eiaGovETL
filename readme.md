# EIA Electricity Demand Pipeline & Dashboard âš¡

![alt text](image.png)

This project provides an end-to-end pipeline to ingest, transform, and visualize electricity demand data from the U.S. Energy Information Administration (EIA). It includes:

- âœ… Python-based ETL pipeline
- âœ… Snowflake warehouse integration
- âœ… React + TypeScript dashboard
- âœ… Dockerized deployment

---

## ğŸš€ Quick Start with Docker

You can run the entire pipeline, backend API, and frontend dashboard using Docker:

```bash
docker-compose up --build
```

## Make sure you set up your env to connect to your Snowflake DB!
```bash
EIA_API_KEY=your_eia_key
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ACCOUNT=your_account.region
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=your_database
SNOWFLAKE_SCHEMA=your_schema
SNOWFLAKE_ROLE=your_role
```
Then visit the live dashboard at:
ğŸ‘‰ http://localhost:3000

EIA API --> Extraction Script --> Cleaned CSV --> Snowflake DB --> FastAPI --> React Dashboard

# Step 1: Data Ingestion

This pipeline begins by pulling **hourly electricity demand data** from the [U.S. Energy Information Administration (EIA)](https://www.eia.gov/opendata/) using their open API.

## Source

We use the endpoint:

```
https://api.eia.gov/v2/electricity/rto/daily-region-sub-ba-data/data/
```

This endpoint provides **regional electricity demand** across the U.S., organized by **RTO/ISO (region)** and **balancing sub-region (subba)**.

---

## How It Works

1. **Authentication**
   Your EIA API key is stored securely in a `.env` file and loaded via Python.

2. **Dynamic Parameters**
   The ingestion service constructs API requests with options like:

   * Frequency (`daily`)
   * Data type (`value`)
   * Sorting (`descending by period`)
   * Pagination (`offset`, `length`)

3. **Request & Save**

   * The script sends a request to EIA's API
   * Parses the JSON payload
   * Saves it to `data/` directory with a timestamped filename

---

## Project Structure

```
eiaGovPipeline/
â”œâ”€â”€ extraction_service/
â”‚   â”œâ”€â”€ config.py                  # Ingestion config
â”‚   â””â”€â”€ eia_ingestion_service.py  # Ingestion script
â”œâ”€â”€ data/                         # Where raw JSON gets saved
â”œâ”€â”€ .env                          # Stores your EIA_API_KEY (not committed)
```

---

## Setup Instructions

1. **Install dependencies**:

```
pip install requests python-dotenv
```

2. **Create `.env` file**:

```
EIA_API_KEY=your_real_key_here
```

3. **Run the ingestion service**:

```
python extraction_service/eia_ingestion_service.py
```

---

## Example Output

```
âœ… Data saved to data/20250701_133219_eia_hourly_data.json
```

---

# Step 2: Data Transformation

Once the raw JSON data is saved, we process it into a clean format for analytics or loading into a database.

## Goal

Transform hourly electricity demand readings into a structured table that maps:

- **Datetime**
- **Region code** (e.g., `ERCO`, `NYIS`)
- **Subregion** (if applicable)
- **Electricity demand value**

---

## How It Works

1. **Load JSON File**
   * The transform script reads the latest file in the `data/` directory

2. **Normalize Structure**
   * Uses `pandas.json_normalize` to flatten nested JSON
   * Extracts only the relevant columns: `period`, `region`, `subregion`, `value`

3. **Clean Data**
   * Converts timestamp strings to proper datetime format
   * Fills missing subregions with 'N/A'
   * Filters out rows with missing or null demand values

4. **Export to CSV**
   * Cleaned data is saved to `data/` with a new timestamped name

---

## File Structure

```
eiaGovPipeline/
â”œâ”€â”€ transform_service/
â”‚   â”œâ”€â”€ config_transform.py     # Paths for transform step
â”‚   â””â”€â”€ clean_data.py           # Main transformation logic
```

---

## Example Output

```
âœ… Cleaned data saved to data/20250701_135037_transformed.csv
```

---

# Step 3: Load to Snowflake

The final step in the pipeline uploads the transformed electricity demand data into **Snowflake**, split into two normalized tables:

- `REGIONS_DIM`: stores region/subregion metadata
- `POWER_USAGE_FACT`: stores time-series demand values

---

## How It Works

1. **Environment Setup**

   The script reads Snowflake credentials from a `.env` file using `python-dotenv`. Required keys:

   ```
   SNOWFLAKE_USER=
   SNOWFLAKE_PASSWORD=
   SNOWFLAKE_ACCOUNT=
   SNOWFLAKE_WAREHOUSE=
   SNOWFLAKE_DATABASE=
   SNOWFLAKE_SCHEMA=
   ```

2. **Data Preparation**

   * Automatically locates the latest transformed CSV in `transformed_data/`
   * Renames columns for Snowflake compatibility
   * Splits data into:
     - `regions_dim` (dimension table)
     - `power_usage_fact` (fact table with `usage_id`)

3. **Normalization**

   * Resets index and uppercases all column names to match Snowflake naming conventions
   * Ensures `write_pandas()` can process the DataFrame

4. **Snowflake Upload**

   * Establishes connection via `snowflake.connector`
   * Uploads each table using `write_pandas()`
   * Tables are overwritten each run (`overwrite=True`)
   * (Optional) Table creation SQL is available in the script as commented-out code

---

## Dependencies

```bash
pip install pandas snowflake-connector-python "pyarrow<19.0.0" python-dotenv
```

> âš ï¸ Make sure `pyarrow` is below version 19 to avoid compatibility issues with Snowflakeâ€™s pandas integration.

---

## File Structure

```
eiaGovPipeline/
â”œâ”€â”€ snowflake_load_service/
â”‚   â””â”€â”€ snowflake_load.py       # Handles data upload to Snowflake
â”œâ”€â”€ transformed_data/           # Contains cleaned CSVs ready for loading
```

---

## Example Output

```
ğŸ“‚ Loading file: transformed_data/20250701_135037_transformed.csv
âœ… REGIONS_DIM uploaded: True 415 rows
âœ… POWER_USAGE_FACT uploaded: True 5000 rows
```

---

# Step 4: Frontend Dashboard (React + TypeScript)

This step introduces a fully interactive frontend dashboard to explore regional electricity usage data queried from Snowflake.

---

## Features

- âœ… Region filter via dropdown (`/api/regions`)
- âœ… Fetches demand data via FastAPI (`/api/data?region=...`)
- âœ… Usage (MWh) bar chart for the most recent 20 records
- âœ… Responsive data table (powered by TanStack Table)
- âœ… Fully typed frontend using React + TypeScript

---

## Tech Stack

- [React](https://reactjs.org/) + [TypeScript](https://www.typescriptlang.org/)
- [Chart.js](https://www.chartjs.org/) via [`react-chartjs-2`](https://react-chartjs-2.js.org/)
- [TanStack Table](https://tanstack.com/table) for tabular view
- [FastAPI](https://fastapi.tiangolo.com/) as API backend
- [Vite](https://vitejs.dev/) for frontend bundling

---

## File Structure

```bash
eiaGovPipeline/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ UsageChart.tsx
â”‚   â”‚   â”‚   â””â”€â”€ DataTable.tsx
â”‚   â”‚   â””â”€â”€ main.tsx
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tsconfig.app.json
â”‚   â””â”€â”€ .env  # Optional: REACT_APP_API_URL=http://localhost:8000
```

## Setup Instructions

Install frontend dependencies
```bash
cd frontend
npm install
```
(Optional) Configure .env

```bash
REACT_APP_API_URL=http://localhost:8000
Run the frontend locally
```
```bash
npm run dev
Then open: http://localhost:5173
```
